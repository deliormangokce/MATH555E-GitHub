{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #2 #Gökçe Deliorman\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #1\n",
    "\n",
    "There are four text files in the data folder\n",
    "\n",
    "* Atatürk's \"Nutuk\" in Turkish\n",
    "* Dicken's novel \"Great Expectations\" in English\n",
    "* Flauberts' novel \"Madam Bovary\" in French\n",
    "* A text file `unknown.txt` in an unknown language\n",
    "\n",
    "Your tasks are\n",
    "\n",
    "* Calculate how many times each character (letter) appear in each text.\n",
    "* Calculate the character distributions, i.e. using the character counts, calculate the probability of each character appearing in the text.\n",
    "* Find the set of characters common to all three texts.\n",
    "* Using the common set and the KL-divergence, show that each language have different character distributions.\n",
    "* Determine the language of the text file `unknown.txt` KL-divergence measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29913 word in Nutuk\n",
      "There are 23777 word in Great Expectations\n",
      "There are 22165 word in Madam Bovary\n",
      "There are 138 word in unknown text\n",
      "nutuk: Counter({' ': 177459, 'a': 119069, 'e': 113769, 'i': 100973, 'n': 78316, 'l': 75077, 'r': 74210, 'm': 70970, 'k': 48302, 'd': 45880, 'ı': 44242, 't': 42763, 'u': 39595, 's': 30823, 'y': 29511, 'b': 29401, '\\x1b': 26262, '[': 26262, 'o': 20304, 'ü': 19764, 'ş': 19547, '\\n': 16872, 'v': 15632, 'z': 15602, '1': 15042, 'g': 14561, '0': 13708, 'h': 12848, ',': 12513, 'c': 10939, 'ğ': 10532, '.': 10469, \"'\": 8956, 'ç': 8621, 'f': 7498, 'p': 7380, 'ö': 7364, '̇': 6358, 'â': 2643, 'î': 1469, '2': 1432, '-': 1214, '\"': 1182, '9': 1132, ':': 1124, 'û': 836, '3': 478, ')': 422, '|': 344, '(': 335, ';': 298, '5': 274, '4': 251, '8': 209, '7': 184, '6': 177, 'j': 163, '!': 150, '/': 82, 'w': 66, '_': 42, 'é': 30, '\\x7f': 22, '$': 10, '·': 6, '«': 6, '»': 6, 'ì': 5, 'í': 4, 'x': 3, 'á': 2, '>': 2, '<': 2, 'ó': 1, '*': 1, 'ú': 1, '`': 1, 'q': 1, '%': 1, 'ß': 1})\n",
      "great expectations: Counter({' ': 171211, 'e': 93720, 't': 70355, 'a': 64021, 'o': 61277, 'i': 55801, 'n': 53781, 'h': 48738, 's': 45936, 'r': 42122, 'd': 37182, 'l': 28564, 'm': 22906, 'u': 22257, '\\r': 20792, '\\n': 20792, 'w': 20526, 'c': 17625, ',': 17198, 'g': 16871, 'y': 16410, 'f': 16130, 'p': 13417, 'b': 12451, '.': 8867, 'k': 7680, 'v': 6896, '“': 3945, '”': 3908, '’': 2596, 'j': 1759, '-': 1295, ';': 1257, '?': 1216, '—': 1151, 'x': 1126, '!': 986, 'q': 710, '_': 494, '(': 267, ')': 267, 'z': 166, ':': 121, '‘': 95, '1': 64, '0': 30, '*': 26, '\"': 22, '[': 19, ']': 19, '4': 14, '2': 13, '/': 13, '8': 12, '5': 12, '9': 11, '3': 11, '7': 10, \"'\": 9, '6': 8, '$': 2, '\\ufeff': 1, '#': 1, '\\t': 1, 'ô': 1, '&': 1, 'ê': 1, '%': 1, '@': 1})\n",
      "madam bovary: Counter({' ': 103394, 'e': 79764, 'a': 46611, 's': 44074, 't': 40678, 'i': 40356, 'n': 36296, 'r': 35857, 'l': 35195, 'u': 33588, 'o': 29092, 'd': 20056, 'c': 16344, 'm': 15786, '\\r': 15609, '\\n': 15609, 'p': 14651, ',': 12533, 'v': 8791, 'é': 8295, \"'\": 7465, '.': 6448, 'b': 6245, 'h': 6177, 'f': 5880, 'q': 5709, 'g': 5190, '-': 4326, 'à': 2810, 'x': 2116, 'j': 2052, 'y': 2029, 'è': 1651, '!': 1513, ';': 1426, 'ê': 1196, 'z': 678, '?': 530, ':': 503, 'ç': 477, 'â': 410, 'î': 328, 'ô': 296, 'ù': 293, 'w': 280, 'û': 241, 'k': 154, '«': 120, '»': 112, '1': 87, '(': 76, ')': 76, '_': 64, 'ï': 37, '/': 31, '5': 28, '*': 28, '\"': 22, '2': 21, '0': 21, '4': 20, '3': 20, '8': 18, '9': 17, '6': 14, '7': 11, 'ë': 9, 'ü': 5, '°': 4, '[': 2, ']': 2, '@': 2, '$': 2, '\\ufeff': 1, '#': 1, '%': 1})\n",
      "unknowntext: Counter({' ': 274, 'e': 165, 't': 126, 'a': 102, 'o': 92, 'h': 80, 'r': 79, 'n': 77, 'i': 68, 'd': 58, 's': 43, 'l': 42, 'c': 31, 'g': 28, 'w': 28, 'f': 27, 'v': 24, ',': 22, 'u': 21, 'p': 15, '-': 15, 'b': 14, 'm': 13, 'y': 10, '.': 10, '\\n': 5, 'k': 3, 'q': 1})\n",
      "Number of characters in Nutuk text file : 1442006\n",
      "Number of characters in great expectations text file : 1035188\n",
      "Number of characters in Madam Bovary text file : 719854\n",
      "Number of characters in unknown text file : 1473\n",
      "probability of letter e in unknowntext: 0.1120162932790224\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import string\n",
    "from urllib.request import urlopen\n",
    "from collections import Counter\n",
    "from re import sub\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "raw = urlopen('https://raw.githubusercontent.com/deliormangokce/MATH555E-GitHub/main/ataturk_nutuk.txt')\n",
    "text1 = raw.read().decode('utf-8').lower()\n",
    "raw = urlopen('https://raw.githubusercontent.com/deliormangokce/MATH555E-GitHub/main/dickens_great_expectations.txt')\n",
    "text2 = raw.read().decode('utf-8').lower()\n",
    "raw = urlopen('https://raw.githubusercontent.com/deliormangokce/MATH555E-GitHub/main/flaubert_madame_bovary.txt')\n",
    "text3 = raw.read().decode('utf-8').lower()\n",
    "raw = urlopen('https://raw.githubusercontent.com/deliormangokce/MATH555E-GitHub/main/unknown.txt')\n",
    "text4 = raw.read().decode('utf-8').lower()\n",
    "\n",
    "##word split\n",
    "#processed = sub('[^a-z ]','',text4).split()\n",
    "\n",
    "\n",
    "def CountWords(url):\n",
    "    raw = urlopen(url)\n",
    "    text = raw.read().decode('utf-8').lower()\n",
    "    processed = sub('[^a-z ]','',text).split()\n",
    "    return len(set(processed))\n",
    "\n",
    "print('There are', CountWords('https://raw.githubusercontent.com/deliormangokce/MATH555E-GitHub/main/ataturk_nutuk.txt') ,'word in Nutuk' )\n",
    "print('There are', CountWords('https://raw.githubusercontent.com/deliormangokce/MATH555E-GitHub/main/dickens_great_expectations.txt') ,'word in Great Expectations' )\n",
    "print('There are', CountWords('https://raw.githubusercontent.com/deliormangokce/MATH555E-GitHub/main/flaubert_madame_bovary.txt') ,'word in Madam Bovary' )\n",
    "print('There are', CountWords('https://raw.githubusercontent.com/deliormangokce/MATH555E-GitHub/main/unknown.txt') ,'word in unknown text' )\n",
    "\n",
    "\n",
    "##count each char\n",
    "from collections import Counter\n",
    "print('nutuk:', Counter(text1))\n",
    "print('great expectations:',Counter(text2))\n",
    "print('madam bovary:',Counter(text3))\n",
    "print('unknowntext:',Counter(text4))\n",
    "\n",
    "#get the length of the data and char count \n",
    "number_of_characters = len(text1)\n",
    "print('Number of characters in Nutuk text file :', number_of_characters)\n",
    "number_of_characters = len(text2)\n",
    "print('Number of characters in great expectations text file :', number_of_characters)\n",
    "number_of_characters = len(text3)\n",
    "print('Number of characters in Madam Bovary text file :', number_of_characters)\n",
    "number_of_characters = len(text4)\n",
    "print('Number of characters in unknown text file :', number_of_characters)\n",
    "\n",
    "\n",
    "##example for e letter: There are 1473 char in unknown text and there are 165 e letter.\n",
    "print('probability of letter e in unknowntext:', 165/1473)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #2\n",
    "\n",
    "For this question consider the [Car Evaluation Data Set](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation) from UCI. Here is the [direct link](https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data) to the dataset.\n",
    "\n",
    "Make [contingency tables](https://en.wikipedia.org/wiki/Contingency_table#:~:text=In%20statistics%2C%20a%20contingency%20table,%2C%20engineering%2C%20and%20scientific%20research.) of the columns (using [`crosstab`](https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html) function from [pandas](https://pandas.pydata.org)) and figure out which pairs of columns are dependent and independent. Explain your result using statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  buying  maint doors persons lug_boot safety carclass\n",
      "0  vhigh  vhigh     2       2    small    low    unacc\n",
      "1  vhigh  vhigh     2       2    small    med    unacc\n",
      "2  vhigh  vhigh     2       2    small   high    unacc\n",
      "3  vhigh  vhigh     2       2      med    low    unacc\n",
      "4  vhigh  vhigh     2       2      med    med    unacc\n",
      "0 buying    high  low  med  vhigh\n",
      "carclass                       \n",
      "acc        108   89  115     72\n",
      "good         0   46   23      0\n",
      "unacc      324  258  268    360\n",
      "vgood        0   39   26      0\n",
      "1 maint     high  low  med  vhigh\n",
      "carclass                       \n",
      "acc        105   92  115     72\n",
      "good         0   46   23      0\n",
      "unacc      314  268  268    360\n",
      "vgood       13   26   26      0\n",
      "2 doors       2    3    4  5more\n",
      "carclass                      \n",
      "acc        81   99  102    102\n",
      "good       15   18   18     18\n",
      "unacc     326  300  292    292\n",
      "vgood      10   15   20     20\n",
      "3 persons     2    4  more\n",
      "carclass                \n",
      "acc         0  198   186\n",
      "good        0   36    33\n",
      "unacc     576  312   322\n",
      "vgood       0   30    35\n",
      "4 lug_boot  big  med  small\n",
      "carclass                 \n",
      "acc       144  135    105\n",
      "good       24   24     21\n",
      "unacc     368  392    450\n",
      "vgood      40   25      0\n",
      "5 safety    high  low  med\n",
      "carclass                \n",
      "acc        204    0  180\n",
      "good        30    0   39\n",
      "unacc      277  576  357\n",
      "vgood       65    0    0\n",
      "[[ 96.    96.    96.    96.  ]\n",
      " [ 17.25  17.25  17.25  17.25]\n",
      " [302.5  302.5  302.5  302.5 ]\n",
      " [ 16.25  16.25  16.25  16.25]]\n",
      "p value is 5.9280625992133936e-36\n",
      "Dependent (reject H0)\n",
      "[[ 96.    96.    96.    96.  ]\n",
      " [ 17.25  17.25  17.25  17.25]\n",
      " [302.5  302.5  302.5  302.5 ]\n",
      " [ 16.25  16.25  16.25  16.25]]\n",
      "p value is 2.5476519845077733e-26\n",
      "Dependent (reject H0)\n",
      "[[ 96.    96.    96.    96.  ]\n",
      " [ 17.25  17.25  17.25  17.25]\n",
      " [302.5  302.5  302.5  302.5 ]\n",
      " [ 16.25  16.25  16.25  16.25]]\n",
      "p value is 0.3202421599003058\n",
      "Independent (H0 holds true)\n",
      "[[128.         128.         128.        ]\n",
      " [ 23.          23.          23.        ]\n",
      " [403.33333333 403.33333333 403.33333333]\n",
      " [ 21.66666667  21.66666667  21.66666667]]\n",
      "p value is 4.039968047270742e-77\n",
      "Dependent (reject H0)\n",
      "[[128.         128.         128.        ]\n",
      " [ 23.          23.          23.        ]\n",
      " [403.33333333 403.33333333 403.33333333]\n",
      " [ 21.66666667  21.66666667  21.66666667]]\n",
      "p value is 1.0294402753134833e-09\n",
      "Dependent (reject H0)\n",
      "[[128.         128.         128.        ]\n",
      " [ 23.          23.          23.        ]\n",
      " [403.33333333 403.33333333 403.33333333]\n",
      " [ 21.66666667  21.66666667  21.66666667]]\n",
      "p value is 2.389155399044034e-100\n",
      "Dependent (reject H0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from scipy.stats import chi2_contingency\n",
    "import scipy\n",
    "\n",
    "\n",
    "headers = [\"buying\", \"maint\", \"doors\", \"persons\",\"lug_boot\", \"safety\", \"carclass\"]\n",
    "\n",
    "#data= pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\", names=['number','buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])\n",
    "data= pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\", header=None, names=headers)\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "\n",
    "crostab_1=pd.crosstab(data.carclass,data.buying)\n",
    "crostab_2=pd.crosstab(data.carclass, data.maint)\n",
    "crostab_3=pd.crosstab(data.carclass, data.doors)\n",
    "crostab_4=pd.crosstab(data.carclass, data.persons)\n",
    "crostab_5=pd.crosstab(data.carclass, data.lug_boot)\n",
    "crostab_6=pd.crosstab(data.carclass, data.safety)\n",
    "\n",
    "\n",
    "mylist=[crostab_1, crostab_2, crostab_3, crostab_4, crostab_5, crostab_6]\n",
    "\n",
    "for index in range(len(mylist)):\n",
    "    print(index, mylist[index], end = \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "for index in range(len(mylist)):\n",
    " stat, p, dof, expected = chi2_contingency(mylist[index])\n",
    " print(expected)\n",
    " print(\"p value is \" + str(p))\n",
    " if p <= alpha:\n",
    "    print('Dependent (reject H0)')\n",
    " else:\n",
    "    print('Independent (H0 holds true)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #3\n",
    "\n",
    "For this question, use [Default of Credit Card Clients Data Set]() from UCI. Here is the [direct link](https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls) to the dataset.\n",
    "\n",
    "Your tasks are\n",
    "\n",
    "* Inspect the dataset.\n",
    "* Would it be appropriate to form a linear regression model to predict the `default payment next month` variable? Explain.\n",
    "* Form a [contingency table](https://en.wikipedia.org/wiki/Contingency_table#:~:text=In%20statistics%2C%20a%20contingency%20table,%2C%20engineering%2C%20and%20scientific%20research.) of the columns `SEX` vs `default payment next month` and `EDUCATION` vs `default payment next month`.\n",
    "* Are there statistically verifiable relationships between credit card defaults, the gender of and the education level the borrower? Which is stronger? Quantify your analysis using [Chi Square Test](https://en.wikipedia.org/wiki/Chi-squared_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
      "0   1      20000    2          2         1   24      2      2     -1     -1   \n",
      "1   2     120000    2          2         2   26     -1      2      0      0   \n",
      "2   3      90000    2          2         2   34      0      0      0      0   \n",
      "3   4      50000    2          2         1   37      0      0      0      0   \n",
      "4   5      50000    1          2         1   57     -1      0     -1      0   \n",
      "\n",
      "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
      "0  ...          0          0          0         0       689         0   \n",
      "1  ...       3272       3455       3261         0      1000      1000   \n",
      "2  ...      14331      14948      15549      1518      1500      1000   \n",
      "3  ...      28314      28959      29547      2000      2019      1200   \n",
      "4  ...      20940      19146      19131      2000     36681     10000   \n",
      "\n",
      "   PAY_AMT4  PAY_AMT5  PAY_AMT6  dpnm  \n",
      "0         0         0         0     1  \n",
      "1      1000         0      2000     1  \n",
      "2      1000      1000      5000     0  \n",
      "3      1100      1069      1000     0  \n",
      "4      9000       689       679     0  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "Default payment next month is a categorical variable. Therefore, it is appropriate to fit a logistic regression model instead of linear regression.\n",
      "dpnm      0     1\n",
      "SEX              \n",
      "1      9015  2873\n",
      "2     14349  3763\n",
      "dpnm           0     1\n",
      "EDUCATION             \n",
      "0             14     0\n",
      "1           8549  2036\n",
      "2          10700  3330\n",
      "3           3680  1237\n",
      "4            116     7\n",
      "5            262    18\n",
      "6             43     8\n",
      "Dependent (reject H0)\n",
      "for gender p value is: 4.944678999412044e-12\n",
      "Dependent (reject H0)\n",
      "for education p value is: 1.2332626245415605e-32\n",
      "Education is stronger than gender\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from scipy.stats import chi2_contingency\n",
    "import scipy\n",
    "\n",
    "data=pd.read_excel(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\", header=1)\n",
    "data.rename(columns = {'default payment next month' : 'dpnm'}, inplace = True)\n",
    "print(data.head())\n",
    "\n",
    "print('Default payment next month is a categorical variable. Therefore, it is appropriate to fit a logistic regression model instead of linear regression.')\n",
    "## default payment next month is a categorical variable.\n",
    "##Therefore, it is appropriate to fit a logistic regression model instead of linear regression.\n",
    "\n",
    "##contingency tables\n",
    "crostab1=pd.crosstab(data.SEX, data.dpnm)\n",
    "print(crostab1)\n",
    "crostab2=pd.crosstab(data.EDUCATION, data.dpnm)\n",
    "print(crostab2)\n",
    "\n",
    "\n",
    "##gender vs education contingency table\n",
    "#crostab3=pd.crosstab(data.EDUCATION, data.SEX)\n",
    "#print(crostab3)\n",
    "\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "stat, p, dof, expected = chi2_contingency(crostab1)\n",
    "if p <= alpha:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (H0 holds true)')\n",
    "print('for gender p value is:', p)\n",
    "\n",
    "stat, p, dof, expected = chi2_contingency(crostab2)\n",
    "if p <= alpha:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (H0 holds true)')\n",
    "print('for education p value is:', p)\n",
    "\n",
    "print('Education is stronger than gender')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #4\n",
    "\n",
    "For this question, use the [Iris Dataset](https://archive.ics.uci.edu/ml/datasets/iris) from UCI.  Here is the [direct link](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data) to the dataset.\n",
    "\n",
    "Your tasks are\n",
    "\n",
    "* Form a [K-NN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) model for this dataset.\n",
    "* Test your model on random samples of your data and calculate its accuracy.\n",
    "* Repeat your calculation 100 times and give an interval of accuracy values leaving the best 2.5% and worst 2.5% accuracy values.\n",
    "* Is there a better way of doing this without repeating the calculation 100 times? Explain.\n",
    "* Find the best parameter $k$ for your dataset for the K-NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=3 accuracy: 93.33333333333333\n",
      "k=5 accuracy: 93.33333333333333\n",
      "90.0\n",
      "96.66666666666667\n",
      "90.0\n",
      "86.66666666666667\n",
      "96.66666666666667\n",
      "90.0\n",
      "96.66666666666667\n",
      "90.0\n",
      "96.66666666666667\n",
      "96.66666666666667\n",
      "90.0\n",
      "100.0\n",
      "96.66666666666667\n",
      "93.33333333333333\n",
      "96.66666666666667\n",
      "86.66666666666667\n",
      "90.0\n",
      "96.66666666666667\n",
      "96.66666666666667\n",
      "90.0\n",
      "93.33333333333333\n",
      "96.66666666666667\n",
      "93.33333333333333\n",
      "100.0\n",
      "86.66666666666667\n",
      "96.66666666666667\n",
      "93.33333333333333\n",
      "96.66666666666667\n",
      "90.0\n",
      "90.0\n",
      "96.66666666666667\n",
      "93.33333333333333\n",
      "96.66666666666667\n",
      "93.33333333333333\n",
      "96.66666666666667\n",
      "96.66666666666667\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "96.66666666666667\n",
      "93.33333333333333\n",
      "90.0\n",
      "90.0\n",
      "96.66666666666667\n",
      "100.0\n",
      "93.33333333333333\n",
      "86.66666666666667\n",
      "93.33333333333333\n",
      "100.0\n",
      "93.33333333333333\n",
      "90.0\n",
      "96.66666666666667\n",
      "96.66666666666667\n",
      "96.66666666666667\n",
      "90.0\n",
      "93.33333333333333\n",
      "90.0\n",
      "96.66666666666667\n",
      "93.33333333333333\n",
      "90.0\n",
      "96.66666666666667\n",
      "86.66666666666667\n",
      "96.66666666666667\n",
      "96.66666666666667\n",
      "90.0\n",
      "90.0\n",
      "90.0\n",
      "93.33333333333333\n",
      "93.33333333333333\n",
      "96.66666666666667\n",
      "93.33333333333333\n",
      "93.33333333333333\n",
      "90.0\n",
      "93.33333333333333\n",
      "96.66666666666667\n",
      "96.66666666666667\n",
      "93.33333333333333\n",
      "96.66666666666667\n",
      "93.33333333333333\n",
      "93.33333333333333\n",
      "93.33333333333333\n",
      "90.0\n",
      "96.66666666666667\n",
      "90.0\n",
      "90.0\n",
      "96.66666666666667\n",
      "93.33333333333333\n",
      "100.0\n",
      "86.66666666666667\n",
      "93.33333333333333\n",
      "96.66666666666667\n",
      "100.0\n",
      "90.0\n",
      "93.33333333333333\n",
      "93.33333333333333\n",
      "90.0\n",
      "96.66666666666667\n",
      "90.0\n",
      "93.33333333333333\n",
      "accuracy mean: 93.46666666666668\n",
      "We can seperate the data set into parts with cross validation. We can calculate the average of accuracy value obtained by this separations.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data=pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header=None)\n",
    "y=np.array(data.iloc[:,4])\n",
    "X=np.array(data.iloc[:,0:3])\n",
    "\n",
    "x_train, x_test, y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=49)\n",
    "\n",
    "k=3\n",
    "model1= KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "model1.fit(x_train, y_train)\n",
    "result=pd.crosstab(model1.predict(x_test), y_test)\n",
    "y_pred1=model1.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred1)*100\n",
    "print('k=3 accuracy:', accuracy)\n",
    "\n",
    "\n",
    "k=5\n",
    "model2= KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "model2.fit(x_train, y_train)\n",
    "result=pd.crosstab(model2.predict(x_test), y_test)\n",
    "y_pred2=model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred2)*100\n",
    "print('k=5 accuracy:', accuracy)\n",
    "\n",
    "###The best k gives us the highest accuracy.\n",
    "\n",
    "\n",
    "## Repeat my calculation 100 times using while loop\n",
    "count=0\n",
    "i = 1\n",
    "while i <= 100:\n",
    " x_train, x_test, y_train, y_test=train_test_split(X,y, test_size=0.2)\n",
    " model= KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    " model.fit(x_train, y_train)\n",
    " result=pd.crosstab(model.predict(x_test), y_test)\n",
    " y_pred=model.predict(x_test)\n",
    " accuracy = accuracy_score(y_test, y_pred)*100\n",
    " count=count+accuracy\n",
    " print(accuracy)\n",
    " i += 1\n",
    "       \n",
    "\n",
    "##accuracy mean \n",
    "print('accuracy mean:', count/100)\n",
    "\n",
    "\n",
    "## We can seperate the data set into parts with cross validation. We can calculate the average of accuracy value obtained by this separations.\n",
    "\n",
    "print('We can seperate the data set into parts with cross validation. We can calculate the average of accuracy value obtained by this separations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #5\n",
    "\n",
    "For this question, we are going to use [Concrete Slump Test Dataset](https://archive.ics.uci.edu/ml/datasets/Concrete+Slump+Test) from UCI. Here is the [direct link](https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/slump/slump_test.data) to the dataset.\n",
    "\n",
    "Your tasks are\n",
    "\n",
    "* Form three separate linear regression model for the following dependent variables:\n",
    "\n",
    "  - SLUMP (cm)\n",
    "  - FLOW (cm)\n",
    "  - 28-day Compressive Strength (Mpa)\n",
    "  \n",
    "* Compare how well these models fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   No  Cement   Slag  Fly ash  Water    SP  Coarse Aggr.  Fine Aggr.  \\\n",
      "0   1   273.0   82.0    105.0  210.0   9.0         904.0       680.0   \n",
      "1   2   163.0  149.0    191.0  180.0  12.0         843.0       746.0   \n",
      "2   3   162.0  148.0    191.0  179.0  16.0         840.0       743.0   \n",
      "3   4   162.0  148.0    190.0  179.0  19.0         838.0       741.0   \n",
      "4   5   154.0  112.0    144.0  220.0  10.0         923.0       658.0   \n",
      "\n",
      "   SLUMP(cm)  FLOW(cm)  Compressive Strength (28-day)(Mpa)  \n",
      "0       23.0      62.0                               34.99  \n",
      "1        0.0      20.0                               41.14  \n",
      "2        1.0      20.0                               41.81  \n",
      "3        3.0      21.5                               42.08  \n",
      "4       20.0      64.0                               26.82  \n",
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:              SLUMP(cm)   R-squared (uncentered):                   0.872\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.863\n",
      "Method:                 Least Squares   F-statistic:                              93.40\n",
      "Date:                Wed, 07 Apr 2021   Prob (F-statistic):                    5.34e-40\n",
      "Time:                        01:30:54   Log-Likelihood:                         -349.06\n",
      "No. Observations:                 103   AIC:                                      712.1\n",
      "Df Residuals:                      96   BIC:                                      730.6\n",
      "Df Model:                           7                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "Cement          -0.0178      0.011     -1.567      0.120      -0.040       0.005\n",
      "Slag            -0.0520      0.014     -3.611      0.000      -0.081      -0.023\n",
      "Fly ash         -0.0223      0.011     -2.122      0.036      -0.043      -0.001\n",
      "Water            0.1710      0.034      5.081      0.000       0.104       0.238\n",
      "SP              -0.2987      0.279     -1.070      0.287      -0.853       0.255\n",
      "Coarse Aggr.    -0.0043      0.006     -0.745      0.458      -0.016       0.007\n",
      "Fine Aggr.       0.0029      0.010      0.306      0.760      -0.016       0.022\n",
      "==============================================================================\n",
      "Omnibus:                        7.379   Durbin-Watson:                   1.812\n",
      "Prob(Omnibus):                  0.025   Jarque-Bera (JB):                6.259\n",
      "Skew:                          -0.513   Prob(JB):                       0.0437\n",
      "Kurtosis:                       2.363   Cond. No.                         460.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:               FLOW(cm)   R-squared (uncentered):                   0.945\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.941\n",
      "Method:                 Least Squares   F-statistic:                              234.3\n",
      "Date:                Wed, 07 Apr 2021   Prob (F-statistic):                    2.03e-57\n",
      "Time:                        01:30:54   Log-Likelihood:                         -405.21\n",
      "No. Observations:                 103   AIC:                                      824.4\n",
      "Df Residuals:                      96   BIC:                                      842.9\n",
      "Df Model:                           7                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "Cement          -0.0263      0.020     -1.344      0.182      -0.065       0.013\n",
      "Slag            -0.1172      0.025     -4.718      0.000      -0.167      -0.068\n",
      "Fly ash         -0.0202      0.018     -1.113      0.269      -0.056       0.016\n",
      "Water            0.4804      0.058      8.277      0.000       0.365       0.596\n",
      "SP              -0.0296      0.482     -0.061      0.951      -0.985       0.926\n",
      "Coarse Aggr.    -0.0237      0.010     -2.359      0.020      -0.044      -0.004\n",
      "Fine Aggr.      -0.0078      0.017     -0.470      0.640      -0.041       0.025\n",
      "==============================================================================\n",
      "Omnibus:                        5.132   Durbin-Watson:                   2.015\n",
      "Prob(Omnibus):                  0.077   Jarque-Bera (JB):                4.015\n",
      "Skew:                          -0.367   Prob(JB):                        0.134\n",
      "Kurtosis:                       2.369   Cond. No.                         460.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                                         OLS Regression Results                                        \n",
      "=======================================================================================================\n",
      "Dep. Variable:     Compressive Strength (28-day)(Mpa)   R-squared (uncentered):                   0.995\n",
      "Model:                                            OLS   Adj. R-squared (uncentered):              0.995\n",
      "Method:                                 Least Squares   F-statistic:                              2841.\n",
      "Date:                                Wed, 07 Apr 2021   Prob (F-statistic):                   2.70e-108\n",
      "Time:                                        01:30:54   Log-Likelihood:                         -242.80\n",
      "No. Observations:                                 103   AIC:                                      499.6\n",
      "Df Residuals:                                      96   BIC:                                      518.0\n",
      "Df Model:                                           7                                                  \n",
      "Covariance Type:                            nonrobust                                                  \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "Cement           0.1056      0.004     26.138      0.000       0.098       0.114\n",
      "Slag             0.0319      0.005      6.222      0.000       0.022       0.042\n",
      "Fly ash          0.0955      0.004     25.507      0.000       0.088       0.103\n",
      "Water           -0.0937      0.012     -7.816      0.000      -0.118      -0.070\n",
      "SP               0.2844      0.099      2.859      0.005       0.087       0.482\n",
      "Coarse Aggr.    -0.0018      0.002     -0.882      0.380      -0.006       0.002\n",
      "Fine Aggr.       0.0172      0.003      5.019      0.000       0.010       0.024\n",
      "==============================================================================\n",
      "Omnibus:                        8.831   Durbin-Watson:                   1.659\n",
      "Prob(Omnibus):                  0.012   Jarque-Bera (JB):                9.088\n",
      "Skew:                           0.563   Prob(JB):                       0.0106\n",
      "Kurtosis:                       3.922   Cond. No.                         460.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "R^2 for fit1:  0.8719605566900663\n",
      "R^2 for fit2:  0.9447088621634644\n",
      "R^2 for fit3:  0.9951960078242847\n",
      "Fit3 has the largest R^2.\n",
      "Amount of significant variables in fit3 are more than others.\n",
      "MSE for fit1: 51.41855823843881\n",
      "MSE for fit2: 152.98388440836567\n",
      "MSE for fit3: 6.531896913453069\n",
      "Fit3 has the minimum MSE\n",
      "The fit3 is the best model\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data=pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/slump/slump_test.data\", header=0)\n",
    "print(data.head())\n",
    "\n",
    "y1=data['SLUMP(cm)']\n",
    "y2=data['FLOW(cm)']\n",
    "y3=data['Compressive Strength (28-day)(Mpa)']\n",
    "X=data[['Cement','Slag','Fly ash','Water','SP','Coarse Aggr.','Fine Aggr.']]\n",
    "\n",
    "model1=sm.OLS(y1,X)\n",
    "model2=sm.OLS(y2,X)\n",
    "model3=sm.OLS(y3,X)\n",
    "\n",
    "fit1 = model1.fit()\n",
    "fit2 = model2.fit()\n",
    "fit3 = model3.fit()\n",
    "\n",
    "print(fit1.summary())\n",
    "print(fit2.summary())\n",
    "print(fit3.summary())\n",
    "\n",
    "print('R^2 for fit1: ',fit1.rsquared)\n",
    "print('R^2 for fit2: ',fit2.rsquared)\n",
    "print('R^2 for fit3: ',fit3.rsquared)\n",
    "print('Fit3 has the largest R^2.')\n",
    "print('Amount of significant variables in fit3 are more than others.')\n",
    "\n",
    "y1_predict= fit1.predict(X)\n",
    "y2_predict= fit2.predict(X)\n",
    "y3_predict= fit3.predict(X)\n",
    "\n",
    "MSE_1 = np.square(np.subtract(y1,y1_predict)).mean() \n",
    "print('MSE for fit1:', MSE_1)\n",
    "MSE_2 = np.square(np.subtract(y2,y2_predict)).mean() \n",
    "print('MSE for fit2:', MSE_2)\n",
    "MSE_3 = np.square(np.subtract(y3,y3_predict)).mean() \n",
    "print('MSE for fit3:', MSE_3)\n",
    "\n",
    "print('Fit3 has the minimum MSE')\n",
    "print('The fit3 is the best model')\n",
    "\n",
    "\n",
    "##second way to find mse\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "#mse = mean_squared_error(y1, y1_predict)\n",
    "#print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
