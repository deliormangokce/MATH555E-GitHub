{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #3 (Gökçe Deliorman)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #1\n",
    "\n",
    "For this question you will use [Olivetti Face Dataset](https://scikit-learn.org/0.19/datasets/olivetti_faces.html).\n",
    "\n",
    "### Part 1\n",
    "\n",
    "1. Split your dataset as train and test subset. But make sure that each test set contains exactly one random image from each distinct individual. This means, you will have to write your own train_test_split function for this dataset.\n",
    "\n",
    "2. Construct an SVM model on your train set, and test its accuracy on your test set. For this part, the images viewed as integer vectors of length 4096 are independent variables while the id number of the person that picture belongs to is the dependent variable. In other words, you are trying to construct an SVM model that recognizes individuals based on their pictures.\n",
    "\n",
    "3. Repeat Step 2 ten times.\n",
    "\n",
    "4. Calculate the mean accuracy and get 95% confidence interval using the t-test.\n",
    "\n",
    "### Part 2\n",
    "\n",
    "Do the same things you did in Part 1 but with a multinomial regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 4096)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= fetch_olivetti_faces()\n",
    "data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
       "        3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,\n",
       "        5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "       11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
       "       13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15,\n",
       "       15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18,\n",
       "       18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20,\n",
       "       20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23,\n",
       "       23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27,\n",
       "       27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
       "       28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30,\n",
       "       30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32,\n",
       "       32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
       "       34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35,\n",
       "       35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39,\n",
       "       39, 39, 39, 39, 39, 39, 39, 39, 39])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x: independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 64, 64)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data.images\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=data.target\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshape x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 4096)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4086</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.309917</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.417355</td>\n",
       "      <td>0.442149</td>\n",
       "      <td>0.528926</td>\n",
       "      <td>0.607438</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.690083</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665289</td>\n",
       "      <td>0.669421</td>\n",
       "      <td>0.652893</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.475207</td>\n",
       "      <td>0.132231</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.157025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.471074</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.557851</td>\n",
       "      <td>0.595041</td>\n",
       "      <td>0.640496</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>0.710744</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.400826</td>\n",
       "      <td>0.491736</td>\n",
       "      <td>0.528926</td>\n",
       "      <td>0.586777</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>0.698347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074380</td>\n",
       "      <td>0.132231</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.128099</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.140496</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.198347</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>0.243802</td>\n",
       "      <td>0.404959</td>\n",
       "      <td>0.483471</td>\n",
       "      <td>0.516529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652893</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.743802</td>\n",
       "      <td>0.764463</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.739669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.582645</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0.648760</td>\n",
       "      <td>0.690083</td>\n",
       "      <td>0.694215</td>\n",
       "      <td>0.714876</td>\n",
       "      <td>0.723140</td>\n",
       "      <td>0.731405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0.173554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.400826</td>\n",
       "      <td>0.495868</td>\n",
       "      <td>0.570248</td>\n",
       "      <td>0.632231</td>\n",
       "      <td>0.648760</td>\n",
       "      <td>0.640496</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.665289</td>\n",
       "      <td>0.698347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388430</td>\n",
       "      <td>0.396694</td>\n",
       "      <td>0.264463</td>\n",
       "      <td>0.099174</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.243802</td>\n",
       "      <td>0.247934</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.351240</td>\n",
       "      <td>0.301653</td>\n",
       "      <td>0.247934</td>\n",
       "      <td>0.247934</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.574380</td>\n",
       "      <td>0.628099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380165</td>\n",
       "      <td>0.334711</td>\n",
       "      <td>0.289256</td>\n",
       "      <td>0.285124</td>\n",
       "      <td>0.338843</td>\n",
       "      <td>0.404959</td>\n",
       "      <td>0.458678</td>\n",
       "      <td>0.487603</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.549587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.533058</td>\n",
       "      <td>0.607438</td>\n",
       "      <td>0.628099</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.632231</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.669421</td>\n",
       "      <td>0.673554</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.190083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.214876</td>\n",
       "      <td>0.219008</td>\n",
       "      <td>0.219008</td>\n",
       "      <td>0.223140</td>\n",
       "      <td>0.210744</td>\n",
       "      <td>0.202479</td>\n",
       "      <td>0.276859</td>\n",
       "      <td>0.400826</td>\n",
       "      <td>0.487603</td>\n",
       "      <td>0.549587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446281</td>\n",
       "      <td>0.392562</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.479339</td>\n",
       "      <td>0.524793</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.574380</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.603306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.516529</td>\n",
       "      <td>0.462810</td>\n",
       "      <td>0.280992</td>\n",
       "      <td>0.252066</td>\n",
       "      <td>0.247934</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.574380</td>\n",
       "      <td>0.615702</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.615702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276859</td>\n",
       "      <td>0.264463</td>\n",
       "      <td>0.293388</td>\n",
       "      <td>0.301653</td>\n",
       "      <td>0.293388</td>\n",
       "      <td>0.322314</td>\n",
       "      <td>0.322314</td>\n",
       "      <td>0.359504</td>\n",
       "      <td>0.355372</td>\n",
       "      <td>0.384298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 4096 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0    0.309917  0.367769  0.417355  0.442149  0.528926  0.607438  0.657025   \n",
       "1    0.454545  0.471074  0.512397  0.557851  0.595041  0.640496  0.681818   \n",
       "2    0.318182  0.400826  0.491736  0.528926  0.586777  0.657025  0.681818   \n",
       "3    0.198347  0.194215  0.194215  0.194215  0.190083  0.190083  0.243802   \n",
       "4    0.500000  0.545455  0.582645  0.623967  0.648760  0.690083  0.694215   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "395  0.400826  0.495868  0.570248  0.632231  0.648760  0.640496  0.661157   \n",
       "396  0.367769  0.367769  0.351240  0.301653  0.247934  0.247934  0.367769   \n",
       "397  0.500000  0.533058  0.607438  0.628099  0.657025  0.632231  0.657025   \n",
       "398  0.214876  0.219008  0.219008  0.223140  0.210744  0.202479  0.276859   \n",
       "399  0.516529  0.462810  0.280992  0.252066  0.247934  0.367769  0.574380   \n",
       "\n",
       "         7         8         9     ...      4086      4087      4088  \\\n",
       "0    0.677686  0.690083  0.685950  ...  0.665289  0.669421  0.652893   \n",
       "1    0.702479  0.710744  0.702479  ...  0.136364  0.157025  0.136364   \n",
       "2    0.685950  0.702479  0.698347  ...  0.074380  0.132231  0.181818   \n",
       "3    0.404959  0.483471  0.516529  ...  0.652893  0.636364  0.657025   \n",
       "4    0.714876  0.723140  0.731405  ...  0.190083  0.161157  0.177686   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "395  0.636364  0.665289  0.698347  ...  0.388430  0.396694  0.264463   \n",
       "396  0.512397  0.574380  0.628099  ...  0.380165  0.334711  0.289256   \n",
       "397  0.669421  0.673554  0.702479  ...  0.194215  0.148760  0.152893   \n",
       "398  0.400826  0.487603  0.549587  ...  0.446281  0.392562  0.367769   \n",
       "399  0.615702  0.661157  0.615702  ...  0.276859  0.264463  0.293388   \n",
       "\n",
       "         4089      4090      4091      4092      4093      4094      4095  \n",
       "0    0.661157  0.475207  0.132231  0.148760  0.152893  0.161157  0.157025  \n",
       "1    0.148760  0.152893  0.152893  0.152893  0.152893  0.152893  0.152893  \n",
       "2    0.136364  0.128099  0.148760  0.144628  0.140496  0.148760  0.152893  \n",
       "3    0.685950  0.727273  0.743802  0.764463  0.752066  0.752066  0.739669  \n",
       "4    0.173554  0.177686  0.177686  0.177686  0.177686  0.173554  0.173554  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "395  0.099174  0.181818  0.243802  0.247934  0.161157  0.157025  0.136364  \n",
       "396  0.285124  0.338843  0.404959  0.458678  0.487603  0.512397  0.549587  \n",
       "397  0.161157  0.161157  0.173554  0.157025  0.177686  0.148760  0.190083  \n",
       "398  0.409091  0.479339  0.524793  0.545455  0.574380  0.590909  0.603306  \n",
       "399  0.301653  0.293388  0.322314  0.322314  0.359504  0.355372  0.384298  \n",
       "\n",
       "[400 rows x 4096 columns]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datax = x.reshape(x.shape[0], x.shape[1] * x.shape[2])   \n",
    "print(datax.shape)\n",
    "\n",
    "df2 = pd.DataFrame(datax)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fb2b1e46a0>"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARkAAAEYCAYAAABoTIKyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAp60lEQVR4nO2db6xd1Xnmn9cGArEx+BobDJgYCCGDIJDEoSVMqjYURDKkoNEwSqSOPCMkPqQzClKl4nSkkTqf+FR1pBmNZDUZPGqmDaHNQFDT1oKiUaWKxvwJYIwxIQ4YX2wwGNuYQIzXfLjHt2s995z1nHXv3eceXz8/Cfks7z9r7bW3F/t99vsnUkowxpiuWLLQAzDGLG68yBhjOsWLjDGmU7zIGGM6xYuMMaZTvMgYYzplTotMRNwaETsj4uWI2DRfgzLGLB5itn4yEbEUwEsAbgawB8BPAHwjpfTC/A3PGHOyc9ocjr0ewMsppVcAICL+AsDtAAYuMsuWLUsTExMDT9ilY+DSpUuLdkRU98+387j4WNVupaVvxVyPn69jAWDJkvLFmc83n/d/sTqZ8nWp6+Ttx48fH7j9o48+qh577Nixgec6fPgw3n///b4PyFwWmYsAvJa19wD4tdoBExMTuOeee6bbasJqE6j25Qf4nHPOKdqnn3560ebJP+OMMwZu43Pn+/Y7N9OySHHfp53Wdst+9atfVY/nvnkhGHbbMJx11llFm+cpv4fqfvIDz6h7VvvH1zInwMx/nKqvuSz8fD/5OnksvP3o0aNF+8MPP5z+ffjw4WLbBx98ULTffvvtgef6/ve/P3DMc3lq+s3MjFUhIu6OiG0Rse29996bQ3fGmJORubzJ7AGwLmtfDGAv75RS2gxgMwCsW7euWITm28zI4f/7qFd1Nqfy/9vwsbxvqxnAbT4+367mhP/PxW9V+f+p+lHrm69TwW8mrf+Hr9EyZ4AeO/8fvva2osapTHGeB+6Lx5LDbxPqrYrfVPn+81j4zai2L19XPrbaHM3lTeYnAK6IiEsj4gwAXwfw8BzOZ4xZhMz6TSaldCwi/iOAvwWwFMB3U0rb521kxphFwVzMJaSU/hrAX8/TWIwxi5A5LTKtRERht8/la5LarmxXZZPndray95W+o7501HSXlk/t/friLzqt+lBtX75O1gPUPKl5GXZbv75Za1DH52NlLYJR86A+G3/sYx8r2u+///70b6Ul1vSbfn1zu6bZqS9TrA/l112bX4cVGGM6xYuMMaZTvMgYYzpl5JpMq8fqIJRt2qqb1I5X+o2ym1t8cvh4pd8o7Un1zdpFPha+DjVnfC41L/ws5PvzscrlXc05ewhz3zXfJHUdSqNRXrn5+ZUnsxpLq09Xrtn98pe/LLax8yzre7mW1JWfjDHGSLzIGGM6ZaTmUkqpeB2ca8Cd6itHuXazS3y+vdXUmqsLfN6fCllgt3Hl2q9oCWlQr+6t4Re1wFA1h8q8Uves9iy2jqXVZaEmIfC5Wj9hq77yz9Lq3wSbU/l2f8I2xiwYXmSMMZ3iRcYY0ykj1WSA0u5rtS+H3cb9APUESf32z7ezK7b6DM+fIdUna6amBzF8Xa2fuGufPFnXUGEDrbqI+izdgrqfc3nW1LG8ne+Juv/5PWtNd6JCNZQrRx7iwEmrmDPPPLNoq/1P4DcZY0yneJExxnSKFxljTKeMXJPJ7fCW8HuFssHZ/mf7kqnZycr+V/ur625xcee20oNUWEHNtX+u9r/yJ6npdbXUj8DM+9niys+o62ZafXCUxlPbt+bnArQnXM/Hzsn2Dx48WLRrGp39ZIwxC4YXGWNMp3iRMcZ0ysg1mRY/gNweVT4UqnwD+7qoNIW19JsqHqiWwmCYvlviaFgPqMVgAW2xTEqbYD1H6T8tKRGUjtWaCkI9H7l2wc+K8jVq7YvJ55H74meJNZbWZ6umk/Gzc+GFFxbt3bt3V881CL/JGGM6xYuMMaZTvMgYYzpl5Ok3c3t3LmVKW+FcGKqcR82XoXUsqtRELa6qxZ4HZpbbaC0V05IeVeWuUfpPrUSq2lf5iyhflNpYeRtfp/I94b5Z41HPXu1cyueGx6bix/Lnq5YvBgBWrlxZtN95553p306/aYxZMLzIGGM6xYuMMaZTRq7JtMQj1VC6COe6YNuUyztwrty8rWJ2WAdhW5ZRvhAtMTytuWpa8tWq6+S+WA9gnURpU/mc87Hc19GjR1GD++Kx1zQ4vn8qzk3Fg/GzxTFC+XZV0qRWwmYYavdf+UGdffbZRds5fo0xY4EXGWNMp8hFJiK+GxH7I+L57O8mImJrROzq/bmydg5jzKnLMJrM/QD+O4D/nf3dJgCPppTui4hNvfa96kTHjx8vSlu21F1qqdnTD7Zt2SeA7c0333xz+rfK2ZpfEwB8/OMfL9ps/7PfBNvs+XaVB0fl+FXxQjyvuRaiNJi56gE8b3n70KFDxTbW2Pj+qfwxrMHx2JcvXz79m+8Pz3m+LzBzTvl+tuSnaakHBmiNraXMscqDze1169ZN/+Y5K/oYuOWfT/z/ALxNf307gC2931sA3KHOY4w5NZmtJnN+SmkSAHp/rhm0Y0TcHRHbImLbkSNHZtmdMeZkpXPhN6W0OaW0IaW0gV8zjTGLn9n6yeyLiLUppcmIWAtg/zAHHT9+vPBvUPZjTbNRGgzrCS0+GkDpI8Db2DZdtmxZ0X7vvfeKNtv0rCewXpBfG49bxcGoOVSxLjXYb4KP5XMrPxqG57kG96Vil9Tzks8zzynrdTwPKldRS45ffm5Zt1L+Pjx2npdaHh6l7/Fznfv71PzfZvsm8zCAjb3fGwE8NMvzGGMWOcN8wv5zAP8I4MqI2BMRdwG4D8DNEbELwM29tjHGzECaSymlbwzYdFNrZyml4pVZfSKrubirY1etWlW0+bOySs+Zf55j1Ks6o9JM8CtszaTkvvnVna9TpYbk1/HcdONXce5LmV68f0vKC54z1vO4rUIY+P7WUmzyuNkcnusn7FoKDA6X4HvAz61K5aCOb0n9ydfN5tMg7PFrjOkULzLGmE7xImOM6ZSRp3rIP8F1mX6TtQlOHXjuuecWbf58l2sCbGOrz+Mq1J+vpZZ+kW1oPhdvV2kJWlJHqjSSaiw8pyqNZb5dld9lzUaVBmGtouYKoEI3GD43P3st6TrUs8LzwO2aO0S/sbS4idSerWrZ34FbjDFmHvAiY4zpFC8yxphOGakms2TJkmpIeE13UZoM24Sc4vC8884r2uwqXtMq+NysB3Dg5/79ZZQF+z6whsM2fD62WspDYKYNzykR2I7m62zxTWopcQvMvC7WLngstQBaldqT/X2UdlFDpVtg3YOfNfar4TmupQJR6TX4OvketZZQqZVjZmo+OU6/aYxZMLzIGGM6xYuMMaZTRqrJpJSqJVjZfqx+exdxFsrXgXWSycnJgW2VbOv5558v2qzZsJ3NNvm7775btCcmJqZ/cwwVt1lrUrFJSpOp2daq9Kvan+8Baxe5nqTSb7711ltFW6Wh4DmvtXfv3l1s4/v76U9/umi/8cYbRZvv9+rVq4v2FVdcUbQ/8YlPTP/m+CCeI9Y0VcyWil1rSQXKczpseSO/yRhjOsWLjDGmU7zIGGM6ZaSazIcffljYu0pXyW0+tv+4zb4Ka9aUuc3ffrssuPDzn/+8aO/YsaNo57FNl156abGN7eKnnnpq4LHAzOvisbFWsW/fvoH78nWwDa/8IlQ+kvzaVNpQVW6D842wfsDaRT5W7uvAgQNFm+eMr+Piiy8u2rnOBcy8h5dccsnAc7FmxnFw77zzTtFmHWTPnj1FOy+3AwC/+MUvpn9ffvnlxTbW4PjZ4jlkbYqp6SwtPjUt+E3GGNMpXmSMMZ3iRcYY0ykj1WTef/99PPvss9NtpbPkWgbrGmw/XnnllUWbdRT2g3nuueeK9vr164v2Zz7zmenfbM+r/LNs07OOwn43rCfl+YnZp+KFF14o2nv37i3a11xzTdHmOVUaTX6tfJ2qBIrKw8yaDMd41WB9iO8/+5Pw/ry95oN17bXXFu1XXnmlaKtcNnwPWCdZu3Zt0X7ttdemf2/fvr3YxlrUhRdeWLT5uVX/TljLynUzVTZa6XmD8JuMMaZTvMgYYzrFi4wxplNGqskcO3as8ClQ+StquU3YfrzssssGHgvM1EE45odt+FpMB/d11113Fe3777+/aLPmonw8ct8IjuHhfTmfDPumsF6kYlny8/E4WVtQ8UKsD7Amw/Fj+dhYS2JNhWOyeCx8v/l+sq9LPs/si/KVr3ylaL/00ktFm6+DNRv2feF7kOtgyjeJfW54ntivRpWeze+RKq87W/wmY4zpFC8yxphO8SJjjOmUkeeTqcVK1PLLqFwztTo6wMza2Oobf+5Hw/lA2G5mPYD9LNiXhbUKHls+R5xHhfOg8HWrfCKqvnU+r3xu1kFYL1AaDt9fpZvV4L6U/sNt1q7ysbIOxhpNHucE6FpHrNkweR5nPhcfy3PKcVP8fLAeWPOjUfmCavWinOPXGLNgyEUmItZFxN9HxI6I2B4R3+r9/UREbI2IXb0/V6pzGWNOPYYxl44B+P2U0lMRcTaAJyNiK4B/D+DRlNJ9EbEJwCYA99ZOtGTJkqYytbXXNzYLVHlWDolX6TrzV0F+tebUDvzKymVM2BWcX92VGZHDZgJflyqpwW02ifLX7dZ0i7yd7xGbuDxP+TywmahSeXJf/MlbfX7P54Gvg81Edhtgc4rntFaOFyivjZ8NPpdyG+DjeXtNdlCpHphhS0fLN5mU0mRK6ane78MAdgC4CMDtALb0dtsC4I6hejTGnFI0aTIRsR7AZwE8AeD8lNIkMLUQAVhTOdQYc4oy9CITEcsB/CWAe1JKh9T+2XF3R8S2iNjWUsXPGLM4GOoTdkScjqkF5nsppb/q/fW+iFibUpqMiLUA+sbtp5Q2A9gMAMuXL0+shcwXbN+z3cy2qkoNcNFFFw3clzUU/uTJn3ZZX2B4rDVbmPWCmq4BzLSzua8VK1YM7FulheC+WS/gsarP6fnnVtYiWPdSLgyqRK5KHZqjwiVYY+G+1CfufN5UOgV+Fvke8XOsUj/UPmErt5F502Ri6qq/A2BHSumPs00PA9jY+70RwEND9WiMOaUY5rXiRgD/DsBzEfFM7+/+EMB9AB6IiLsAvArgzk5GaIw5qZGLTErpHwAMepe8aX6HY4xZbIw8rKBmfzJqew7b4KzBsF7Atiofn9vZbO8zfC7lm6L0gNymZ3ufx6JSf7IdzXoBny/3CVJ+MTV/D2DmdbG/SW1sfF0qHIK383W1bFdaA18XX7cqS1ILaeE55zlWflJ8nerZzFF+MSoMaOBxQ+1ljDGzxIuMMaZTvMgYYzpl5Ok38/IgysarhZJzm/0D2O5VsS81fxL20eC+VekQtrOVD0dNC2EtoTZn/bbzPHE7HzvPoYqxYvuf54H9RWraBffFx3KbNThVvrcWR6XuL8N9M0q7qN1vlRaWn2NFTZPh+6f8ZpSGM33ckGMzxphZ4UXGGNMpXmSMMZ0yUk3m+PHjRYrFlvgRtW9r7gtu14I3lfbA21VJVZWWMrfRVZkKtV35h/D+ud+F0rWUBqPGWtN4lIamtAkei9I28nvCc8S+KEoHYw2H9+fzt+QPUv8O1D2obVcljWsxXE6/aYxZMLzIGGM6xYuMMaZTRq7JqPIQg1C+BpwnRZVIUT4fuT2qbFUV46PKffJYcztcxcEonYNRdna+XfkSKY1G6WS8vaZNtGpR6p7V/EtUTl41L0rv4/udj03NsXpuVZ7lGryvihfL+67eu6FHYIwxs8CLjDGmU7zIGGM6ZeT5ZHK7ryVfjIJzlSi7Wdm2NR2F91W1cNjWZbu5ZtPzNr4urgnF1PQeYKb/yLB5W4GZNjv7h3DfHOPD/ic5rLGp+6WeJVXOt+Yn1VIXq9/+fJ1KF2tB5cxWz1re5nFwu+ajYz8ZY8yC4UXGGNMpXmSMMZ0yUk0mIgrbWMUT1WxflU9ExRepGJ7asa16gcoXU7Ob2X7n62S/IxWjw7lsWuoNKX8gBc8L6yL5PVX1pPke1Pyc+rVb5rzmH9JvbMpXpVa/WuW9Uf5ePNaWPMyqr1reHPvJGGMWDC8yxphOGam5dNppp2FiYmK6rT4F1tyWuVwnvyaq1202p1o+p6tz8ydL9QpcM2m4BK4yj5QLvErXmM8jvx6zicqv3nzd7FagSgfn58ufE0DPGZskLc8W9z3fKJeGYT8FA9p9glHm06Bx9DtWpY0YhN9kjDGd4kXGGNMpXmSMMZ0yUk1myZIlRekJpvYJW6UVUC7t/cZSo+omLY5tdXnnz8q5ttGaylHpQarMRd7mOVafYnksqs3ny0MkWDtYsWJFtW81VlW+N2/zOFmbYG1J6Xvqc7p6nmrnVs+acuWo6UHq2Rk2BMVvMsaYTvEiY4zpFLnIRMSZEfFPEfHTiNgeEX/U+/uJiNgaEbt6f67sfrjGmJONYTSZDwB8OaV0JCJOB/APEfFjAP8awKMppfsiYhOATQDurZ1o6dKlM2zrnJqvAtt/qnRoaxrKWrpG5Vui9AA+tyoVe/Dgwenf7GvC/kGt2pNKLZmPncfdWpaWj1elRfLz56Vz+o1T6Ri8XelJ+flZc1HhFEqDUdpFvn/rs9SaMrOm2ahUna1a4wnkm0ya4sQdP733XwJwO4Atvb/fAuCOoXo0xpxSDKXJRMTSiHgGwH4AW1NKTwA4P6U0CQC9P9cMOPbuiNgWEduUd6IxZvEx1CKTUvoopXQdgIsBXB8RVw/bQUppc0ppQ0ppgzI7jDGLjyY/mZTSwYh4HMCtAPZFxNqU0mRErMXUW06VuZREYdhmf+utt4q2sm2VfZlrHao0qPLR4bGoch2HDx+e/s16DWsyKqaHt3NMEMdG5VoFayiqbC23WdtgajFbBw4cKLbxHLNvUas/kNKyaseqUrDqjb2ldAyfS6UJmUuZGn4JqMWWAcP79wzzdWl1RJzb+30WgN8G8CKAhwFs7O22EcBDQ/VojDmlGOZNZi2ALRGxFFOL0gMppUci4h8BPBARdwF4FcCdHY7TGHOSIheZlNKzAD7b5+8PALipi0EZYxYPI0+/2RKnUYNzm7z00ktF+7bbbivaKrVgTdvgbWyrqnMpTaZWYleVROF5YL8ahrUN3p/Pl8MaCo9b5c1R6TfzeWItSpVTUXqBiunJtY+WsjD9zs0of6OaH46aU74O9e+rppPx/VCpO4fFYQXGmE7xImOM6RQvMsaYThl5mdr5yqXKtuirr75atNn/g/1L2I6uOQqqeJG52uSsk5x77rkDz53HNQHAZZddVrT37y/dlVjL4OP5/LlGo2Jw+FieYxVvxCV2c/+T1pwsqlyrKkOc96ecRpW2pHyV+Hmo+dWw9jTXGC72D8rHqvIeK1+kgWMYai9jjJklXmSMMZ3iRcYY0ykj95PJ7dGWWkcK1h5ef/31ov2pT32qenzNp6OlxCmgY50YPn+uJ3GczDPPPFO0WYtYvXp10Vb6AesB+fY333yz2KbqKLHGkmtLAHDVVVcV7ZovDM8Zz0PrHCsfnxx1v9X9b9Xw8uNb8+q2xirVdBalHdZK3FZzYg/cYowx84AXGWNMp3iRMcZ0ysj9ZFQe0WFh+5Ht/8nJyaLNmgz7C9R0FWUXq/gSBfs+5LlUlK/C9u3bizZf5/Lly4u2yj+S+9mw/87KlWWueD6Wc7ywLsZ2+yWXXFK0c92F9R0eN2sPrO+w5tKSC0Xl9FV1llrb+bNY0z36tZUeqPSjXJNTulYtb3It3stvMsaYTvEiY4zplJGaS13Cr2ucjpM/gbK7du1zHW9TpT3U2LjN5lKeInPfvn3FNjZZ2KTZs2dPdX/+xM19n3POOdO/uaSwuk42zfj4d999t2hzKEgO36/169cXbWWytIYZ5G2VTlOZU7w/m3I81tzsUKa2KjvbOva8rZ5TxmEFxpixwIuMMaZTvMgYYzplQcMKWlD2Idu5bP+rT3+1lIjq2NYQeNYbauVa2J7n+Vu1alX1XNzm8AtOt5nrTfwZWLn6s/3PfbHbQG3OWSvieWhNO8nUwi34/qk0lHNN15lfC89Za9oJ9ezVSgOrc822OKPfZIwxneJFxhjTKV5kjDGdMnI/mVb7dbbs3r27aLPLO+sJtTAD5aqtbFmlXfD23KVelURl2FeFfXp4HvhaauH7ykZn/UfpXjzn+flYg2Ftgq+rNRVEbR7Z94h1K/U8qHtW8zdSulXNx6bf9pb9W9JC9DvXIPwmY4zpFC8yxphO8SJjjOmUkWsyuV03F31GaSp79+4t2hzLxDE8NZQOUvM9AGbarux/wteSb1elQ1XJ01bfh3y70ly4rfxHVDvXWfg6lb+I8lVS85Yfz/eH9R+Gz8U+PHx/azpYq7+PiuFq1claqPnYFH3OugdjjBkCLzLGmE4ZepGJiKUR8XREPNJrT0TE1ojY1ftzpTqHMebUo0WT+RaAHQBW9NqbADyaUrovIjb12vfWTpBSqsY/tJRIUbYo29E7d+4s2kqTqfkPKF8EtsE5HoXPx6VGcvtW+WTUyo7260v5bOTbeQ45JSbD9n9rfFHuG6N0kNYYHVWmJB/bihUrim1qTpWuofSi/B4q3ar1WeR7UIuTU9fFz/WwDPUmExEXA/hXAP40++vbAWzp/d4C4I5ZjcAYs6gZ1lz6EwB/ACBf6s5PKU0CQO/PNf0OjIi7I2JbRGyb7UpojDl5kYtMRNwGYH9K6cnZdJBS2pxS2pBS2qBSIhpjFh/D/Ku/EcDvRMRXAZwJYEVE/BmAfRGxNqU0GRFrAeyvngVT9mJXC43Km/uzn/2saH/pS18q2jWtgnUPtmuVBqO21/ws+Dp4/lSJE76uI0eOFO1a/lk+Vt071ts45oeplVhlbYHjiVizUfPQ8hbN51bnUqViVR7emhZZ01D6oTSb2thbtaZ5K1ObUvp2SunilNJ6AF8H8FhK6XcBPAxgY2+3jQAeUucyxpx6zMVP5j4AN0fELgA399rGGFPQZLuklB4H8Hjv9wEAN83/kIwxi4mRK7E1261mA7bGB3FelYMHDxZtjk9pyT3MfbNeUPOD6Nc353jJj1dxMDwWpQepOJpcj+BxK10jr9kEzNRkVDxRPi9qzvg6lU8Oj73mP6J0LxUfxKh4oXxe1P1RsUgtcVK8P8/50aNHq+Me1q/NYQXGmE7xImOM6RQvMsaYThmpJpNSqtr1Lbk0lO8B2/CHDh0q2hwvxPZore6S8pNQtut5551XtGt5W5Tdq7QqpdnwPOXXovLHsj+Jysur9IFcC1EamfI1UfpRLbaJr5ufDUb5JinflXx7aw5f9aypvmt5dBTOJ2OMGQu8yBhjOmWsgolq4fvqFVS5wO/bt69os7nE4f21z4qtbuVnn31201jzV082G/iVlj8zqlf7llQC6lWbzTw2n5Spx9dWM5f5unjO1OdxpmZWKPNXhVcod/yaiaP6Uq4XLaZZv3btWGbY1J1+kzHGdIoXGWNMp3iRMcZ0ykg1mYhoct+v2apKo2FblrWLF198sWhfcMEFA8/X6jbOYQL8aZfh7fln5jfeeKPYxikwOaSBYbuatQrWpmr7MjznHLrBc85hBrXSs62lgVs/G6u0ljmsg9RKGvfbv6W8qwpBaEmf2q+v2vmUXqP0vEH4TcYY0yleZIwxneJFxhjTKWMVVtBSEoVhm519OFauLMtCPfbYY0X7hhtuKNq5rqLKb7D+w9oD28lvv/120Wa7Odc2Jicni22sobz77rtF+/Dhw9WxcJv1g/xali1bhhqsqSg/Gp4n1qJyfyJO1aHmlNv8PHDfPLZ8f5VGhJ9hpYswtTImKpWD0p5ay87m51Nlh2spUas+bk0jMsaYRrzIGGM6xYuMMaZTRq7JKJ+TQaiYnH595bD9z/4mrH1cfvnl079rpXUBnZaA/UVef/31ol1LQ8EaC88fayocP8TzwJoOX1t+vgMHDhTbWlNcqP35nuRjZw2NywpzPBhrLip+jPWk/PlSeo5Kr6B0k1pKTZVulVEaDo+V73etTO1cfIuKMQ21lzHGzBIvMsaYTvEiY4zplJFqMkuWLJmhGeRUU/g1+geodIy8/65du4r2+vXrp38rzaVWXgOYmfJSbc/HxnluuC+OXVJ2s2rnY1M+Gq3XzWNjrSrfn/Ug1q14LOzTw3rPqlWrivaaNWuKdu6Hwz46KndRK7XnvLXsSGu8EZP/u2C9Rvk95X05/aYxZsHwImOM6RQvMsaYTllQP5mWGI/W8p2tpUKefPLJon3LLbcMPHdrGVPWB9atW1e0WXfJfXiOHDlSbGNNi/2HVNyN8pvI7Wy+Lr4OlT9W6WA1vxqVB0fpJLw/j71WariW56bf2Fp1sBrq30RrThd1D2q5jfnZ4HZ+bmsyxpgFY6g3mYjYDeAwgI8AHEspbYiICQDfB7AewG4A/zal9E43wzTGnKy0vMn8VkrpupTShl57E4BHU0pXAHi01zbGmIK5aDK3A/jN3u8tAB4HcG/tgIiQNWtyavWHZhsDdQK2fV9++eWivXPnzunfV1xxRbGtVl4VmGmfsm7Cx3NO4Nz2ZU2GfU9UXSaOVar5xXBb+cXwddTyBffbv+Z/pErc1mKP+vXF94T9anIdjH2P1HW35uFlarms1f1SZW2V30y+Pz87rOfU4ubmQ5NJAP4uIp6MiLt7f3d+Smmy18EkgDUDjzbGnLIM+1pxY0ppb0SsAbA1Il6UR/ToLUp3A+2R1MaYk5+h3mRSSnt7f+4H8EMA1wPYFxFrAaD35/4Bx25OKW1IKW1oMZWMMYsD+a8+IpYBWJJSOtz7fQuA/wrgYQAbAdzX+/Oh+R5czU9GxTIp2IZkXeSRRx6Z/v3Nb36z2KZytrTWDGJbN4+j4etUmgz7MnAelZqvA4+lVQ9QWgWPtaaj8P+QWnP6Mi31ylt1D1WPmqnlfKlpZP1o9dGpxZOpY/nZybWrmiYzzKvF+QB+2Jvo0wD8n5TS30TETwA8EBF3AXgVwJ1DnMsYc4ohF5mU0isAru3z9wcA3NTFoIwxi4exKolSQ7lbq0/c6hWXX4nzdJwvvPBCse3zn/980VZhBsrEYfLXVFUqlq+LTbm5vLq3pp1ULu6qREo+VpVGUvWlPivX5k2lDW0tmcvUXP1bU5gyKoVmbSzcF98vfo6HxWEFxphO8SJjjOkULzLGmE45aRxXlP2v0g4ofaFmCz/xxBPFtquvvrpoKyfD1hCIXIdRegDbybWSF3zufttrruIqnQKj3Axa3BDU/VfpOFrOpz7lKrcB9SzWPomrY1s/WddK3nDffCzvy30P6/fmNxljTKd4kTHGdIoXGWNMp5w0mowqx8n2oyrXoTSdXG94+umni22cqvOLX/xi0c7DAoCZKRA4hEH5fNTgtJKsD3HfLaEByl1ehXqoNpNv532Vf9VcNZpcu1A6CKeCYFQKDH4+WsIKlB+N0mxqJZf535Cah9y3qHZv/SZjjOkULzLGmE7xImOM6ZSTRpNRsUls56o4CxUDktu6XLb0wQcfLNqrV68u2ldeeWXRZjtY6QU1G32u6RR4XmralUrlyNelNBzlJ1PTTVrjhZSe0FIqVvnBcN813UNtb02/ye1aiRtgpoZTi11iVErUQfhNxhjTKV5kjDGd4kXGGNMpI9VkIqKw45R/SG5PKu2h1Y+ilssEKG1dVXZk69atRXvlypVFe2JiotoXU0tDqa5bnVuV98g1G6UHMKqkhvKbye9RSxmRfn3VrguY6V+UH9/q36P6bimZrPxeeHstFgnQGg0fn8PXzbmK7CdjjBkLvMgYYzrFi4wxplMW1E9G2a618p2ttJZQyftWZWk5BzBrNF/72teKNvv0MPn5lTahYrDY5lZaRT4vKueriqtRvi01Hx1VdlblAGb4+LwsLY9F+dS0alUqR0w+r636D98/Vb6l1ub7xW3WZPIYPGsyxpgFw4uMMaZTvMgYYzplrOsu5falssFVrIqyi2s2pbK5WT947LHHivb5559ftL/whS8Ubc4vk/fH42L/Dr7OQ4cODTwXMDP+pKaz8LEq52trDpjanCutQWkwyr9I+Zfk8LOybNmyal+MimWq5dnl3DVKJ1M6J5PPC98Pfq5ZkxlWO/SbjDGmU7zIGGM6xYuMMaZTxjqfTM1mb80XomzTmt+MqpvDbbbZf/zjHxdt9pO55pprinZu67JdrHxTWmuG13LAqHwxSnNR9cZreXRYM2mtbaT0oVptbD433wM1x5zTV81TLX6otg3Q9cXUc1971lj/Y9S5T+A3GWNMpwy1yETEuRHxYES8GBE7IuKGiJiIiK0Rsav350p9JmPMqcaw5tJ/A/A3KaV/ExFnAPg4gD8E8GhK6b6I2ARgE4B7ayfhVA9zQX3CbH1dnkuJVNU3vz7/6Ec/Ktr8ivvJT35y+rf6BM1wqlCVMpNfkfO2eu1vCQsBZo69liK1NTVHa8ncmqmmroNNGL4OlW6hJb2GQn2qV+Z0fjzPIbtW8Lnn7RN2RKwA8BsAvtMb9IcppYMAbgewpbfbFgB3qHMZY049hjGXLgPwJoD/FRFPR8SfRsQyAOenlCYBoPfnmn4HR8TdEbEtIra1OOIZYxYHwywypwH4HID/mVL6LID3MGUaDUVKaXNKaUNKaYPyjDTGLD6G+Ve/B8CelNITvfaDmFpk9kXE2pTSZESsBbBfnag1rCBflFpSM5zoK4dtUfX5LR+nWhzVp1nmyJEjRfsHP/hB0b7zzjunf19++eXFNtYD2I5udfWvlZbhT/F8LKdLUDrX0aNHq9trn4bV/eV5UOk5WYvKz8faA7v2K61KlZKptXkOlBtA7Tr67V/TRNm1oqbBtCDfZFJKbwB4LSJOFBO6CcALAB4GsLH3dxsBPDSrERhjFjXDLk3/CcD3el+WXgHwHzC1QD0QEXcBeBXAnZXjjTGnKEMtMimlZwBs6LPppnkdjTFm0THykig1f4a5ptjMUekZlGt4zf5U41S6E5/74MGDRfvll1+e/n3BBRcU29hOZtdv5Q/COkvtWvjcquSJ8gdR+kLeVroGX1dr6Rgm14v4XKzJsF+MmhcVlpL3p8qQKM1NhdfU5onnTPkeDYvDCowxneJFxhjTKV5kjDGdMlbeca0xQrV9a7Ep/WA7umYn1/bt15fSJtjv4pxzzhl4Lk6vyXoBo0qL1FI9tKQJ6HcuHhuPhbWMalkNoTWxhqOepVr80eHDh6v7qr54u/LRyudF+fsoDUZdd60cD98fnuPZeuz7TcYY0yleZIwxneJFxhjTKTGfvimys4g3AfwCwHkA3hpZx8MzruMCPLbZMK7jAhbf2D6RUlrdb8NIF5npTiO2pZT6eRAvKOM6LsBjmw3jOi7g1BqbzSVjTKd4kTHGdMpCLTKbF6hfxbiOC/DYZsO4jgs4hca2IJqMMebUweaSMaZTRrrIRMStEbEzIl7ulVFZMCLiuxGxPyKez/5uwWtJRcS6iPj7Xn2r7RHxrTEa25kR8U8R8dPe2P5oXMbWG8fSXrL7R8ZsXLsj4rmIeCYito3Z2DqvqTayRSYilgL4HwC+AuAqAN+IiKtG1X8f7gdwK/3dJkzVkroCwKNoSJg+jxwD8PsppX8B4NcB/F5vnsZhbB8A+HJK6VoA1wG4NSJ+fUzGBgDfArAja4/LuADgt1JK12WfhsdlbCdqqn0awLWYmr/5HVtKaST/AbgBwN9m7W8D+Pao+h8wpvUAns/aOwGs7f1eC2DnQo6vN46HANw8bmPDVIG/pwD82jiMDcDFvX8QXwbwyDjdTwC7AZxHf7fgYwOwAsDP0dNmuxrbKM2liwC8lrX39P5unBiqltSoiIj1AD4L4AmMydh6JskzmKpOsTVNVbEYh7H9CYA/AJCnoRuHcQFAAvB3EfFkRNw9RmObU021YRnlItMvb4A/bQ0gIpYD+EsA96SUDqn9R0VK6aOU0nWYenO4PiKuXuAhISJuA7A/pfTkQo9lADemlD6HKang9yLiNxZ6QD3mVFNtWEa5yOwBsC5rXwxg7wj7H4Z9vRpSGLaWVBdExOmYWmC+l1L6q3Ea2wnSVKnixzGlay302G4E8DsRsRvAXwD4ckT82RiMCwCQUtrb+3M/gB8CuH5Mxtavptrn5ntso1xkfgLgioi4tFda5euYqt00Tix4LamYyjr0HQA7Ukp/PGZjWx0R5/Z+nwXgtwG8uNBjSyl9O6V0cUppPaaeq8dSSr+70OMCgIhYFhFnn/gN4BYAz4/D2NKoaqqNWGj6KoCXAPwMwH8etdBFY/lzAJMAfoWpFf0uAKswJR7u6v05sQDj+peYMiOfBfBM77+vjsnYPgPg6d7YngfwX3p/v+Bjy8b4m/hn4XfBx4Up3eOnvf+2n3jux2FsvXFcB2Bb757+XwAr53ts9vg1xnSKPX6NMZ3iRcYY0yleZIwxneJFxhjTKV5kjDGd4kXGGNMpXmSMMZ3iRcYY0yn/H3xzOe5SFqoZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage.io import imshow\n",
    "image_fromdata = x[5]\n",
    "imshow(image_fromdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each row is a face image corresponding to one of the 40 subjects of the dataset. So there are 40 classes in the dataset. Each test set should contain exactly one random image from each distinct individual. To achieve this, I used the stratify feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 4096) (80, 4096)\n",
      "(320,) (80,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df2, y, test_size=0.2, stratify=y)\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9625\n",
      "0.9625\n",
      "0.8875\n",
      "0.9375\n",
      "0.9375\n",
      "0.9375\n",
      "0.9\n",
      "0.9625\n",
      "0.95\n",
      "0.9125\n",
      "sum: 9.35\n",
      "mean is: 0.9349999999999999\n"
     ]
    }
   ],
   "source": [
    "sum=0\n",
    "for i in range(10):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df2, y, test_size=0.2, stratify=y)\n",
    "    model = SVC()\n",
    "    model.fit(x_train,y_train)\n",
    "    print(model.score(x_test,y_test))\n",
    "    sum=sum+model.score(x_test,y_test)\n",
    "\n",
    "print('sum:', sum)  \n",
    "print('mean is:', sum/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: multinomial regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gokcedeliorman\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm=linear_model.LogisticRegression()\n",
    "\n",
    "lm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gokcedeliorman\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "fit= lm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9625"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, lm.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 23, 39,  6, 38, 28, 19, 22,  5, 24, 37,  7, 34, 32,  8, 31, 25,\n",
       "       34, 27, 29, 24,  5,  2, 26, 11, 26, 11, 33,  5, 35, 35, 13, 39,  4,\n",
       "        2, 10, 30, 36, 15, 17,  9, 29,  9,  1, 14,  7, 14,  1,  6,  3, 15,\n",
       "       30,  0,  3, 27, 16, 20, 32,  0, 28, 21, 25, 19, 38, 22, 16, 33, 31,\n",
       "       17, 23, 13, 10, 37, 20,  8,  5, 21, 36, 18,  0])"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gokcedeliorman\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gokcedeliorman\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gokcedeliorman\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gokcedeliorman\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gokcedeliorman\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gokcedeliorman\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gokcedeliorman\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gokcedeliorman\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gokcedeliorman\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.975\n",
      "0.95\n",
      "sum: 9.624999999999998\n",
      "mean is: 0.9624999999999998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gokcedeliorman\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "##10times\n",
    "\n",
    "sum=0\n",
    "for i in range(10):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df2, y, test_size=0.2, stratify=y)\n",
    "    lm=linear_model.LogisticRegression(multi_class='multinomial')\n",
    "    lm.fit(x_train, y_train)\n",
    "    metrics.accuracy_score(y_test, lm.predict(x_test))\n",
    "    print(metrics.accuracy_score(y_test, lm.predict(x_test)))\n",
    "    sum=sum+metrics.accuracy_score(y_test, lm.predict(x_test))\n",
    "\n",
    "print('sum:', sum)  \n",
    "print('mean is:', sum/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #2\n",
    "\n",
    "For this question you will use [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "### Part 1\n",
    "\n",
    "Convert the dataset into numerical data using [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) from SciKitLearn's `sklearn.feature_extraction.text` module. Make sure that you also record whether a given movie review is positive or negative or neutral. Calling on `CountVectorizer` on individual entries is not going to be enough. You will have to do some preliminary work. Read the documentation carefully.\n",
    "\n",
    "### Part 2\n",
    "\n",
    "Using the numerical data you constructed in Part 1, construct an LDA model to see if data projects into a 2D space with clear separation. Analyze your result.\n",
    "\n",
    "\n",
    "### Part 3\n",
    "\n",
    "Using the numerical data you constructed in Part 1, \n",
    "\n",
    "1. Split the data as train and test using SciKitLearn's `train_test_split` function.\n",
    "2. Form a multiclass SVM model on the train set and test its accuracy.\n",
    "3. Repeat a small number of times and get mean accuracy and its error band.\n",
    "\n",
    "### Part 4\n",
    "\n",
    "Repeat Part 2 using multinomial regression models instead of SVM.\n",
    "\n",
    "### Part 5\n",
    "\n",
    "Using the numerical data you constructed in Part 1, \n",
    "\n",
    "1. Construct an PCA model and look at the eigenvalues from largest to smallest. \n",
    "2. How many dimensions needed to capture 90% of the variation of the data? (Read the documentation of [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) form SciKitLearn)\n",
    "3. Transform your data using the result you obtained in Step 2.\n",
    "4. Construct an SVM model on the new dataset you constructed and cross-validate it.\n",
    "5. Compare your result with the result you obtained in Part 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from urllib.request import urlopen\n",
    "from collections import Counter\n",
    "from re import sub\n",
    "import urllib.request\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When I run the following command, text-train folders were created on my computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I couldn't delete the unsup folder using code, so I deleted it manually from the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg = load_files(\"C:\\\\Users\\\\gokcedeliorman\\\\Desktop\\\\aclImdb\\\\train\\\\neg\")\n",
    "train_pos=load_files(\"C:\\\\Users\\\\gokcedeliorman\\\\Desktop\\\\aclImdb\\\\train\\\\pos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I couldn't upload the dataset to github as it is too big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= load_files(\"C:\\\\Users\\\\gokcedeliorman\\\\Desktop\\\\aclImdb\\\\train\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, Y_train = train.data, train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 25000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b\"The Movie was sub-par, but this Television Pilot delivers a great springboard into what has become a Sci-Fi fans Ideal program. The Actors deliver and the special effects (for a television series) are spectacular. Having an intelligent interesting script doesn't hurt either.<br /><br />Stargate SG1 is currently one of my favorite programs.\""
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"type of text_train: {}\".format(type(text_train)))\n",
    "print(\"length of text_train: {}\".format(len(text_train)))\n",
    "text_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"The Movie was sub-par, but this Television Pilot delivers a great springboard into what has become a Sci-Fi fans Ideal program. The Actors deliver and the special effects (for a television series) are spectacular. Having an intelligent interesting script doesn't hurt either.  Stargate SG1 is currently one of my favorite programs.\""
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\n",
    "text_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in test data: 25000\n",
      "Samples per class (test): [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "test = load_files(\"C:\\\\Users\\\\gokcedeliorman\\\\Desktop\\\\aclImdb\\\\test\\\\\")\n",
    "text_test, Y_test = test.data, test.target\n",
    "print(\"Number of documents in test data: {}\".format(len(text_test)))\n",
    "print(\"Samples per class (test): {}\".format(np.bincount(y_test)))\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect.fit(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 74849\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "words = vect.transform(text_train)\n",
    "print(format(repr(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\gokcedeliorman\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]    |     ...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all', halt_on_error=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way to get data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2017) # for reproducibility\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_words` argument in `load_data` has been renamed `num_words`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\gokcedeliorman\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gokcedeliorman\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80 # cut texts after this number of words (among top max_featuresmost common words)\n",
    "batch_size = 32\n",
    "#(X_train, Y_train), (X_test, Y_test) = imdb.load_data(nb_words=max_features)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (I copied this part from lecture 7 but it doesnt work.:( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-409-704ae6f5ea03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearDiscriminantAnalysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#plt.scatter(transformed[:,0],np.zeros(len(transformed)),c=y_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[0mTarget\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \"\"\"\n\u001b[1;32m--> 424\u001b[1;33m         X, y = self._validate_data(X, y, ensure_min_samples=2, estimator=self,\n\u001b[0m\u001b[0;32m    425\u001b[0m                                    dtype=[np.float64, np.float32])\n\u001b[0;32m    426\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    796\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m         array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    576\u001b[0m                                       \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                                       \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[1;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[0;32m    354\u001b[0m                         \u001b[1;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m                         'convert to a dense numpy array.')\n",
      "\u001b[1;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "model = LinearDiscriminantAnalysis()\n",
    "model.fit(words, Y_train)\n",
    "transformed = model.fit_transform(words, Y_train)\n",
    "#plt.scatter(transformed[:,0],np.zeros(len(transformed)),c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
